#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è„‰è„‰çˆ¬è™«
çˆ¬å–è„‰è„‰å¹³å°çš„å†…æ¨ä¿¡æ¯
"""

import re
import json
from typing import List
from base_crawler import BaseCrawler, JobData

class MaimaiCrawler(BaseCrawler):
    """è„‰è„‰çˆ¬è™«"""
    
    def __init__(self):
        super().__init__('è„‰è„‰')
        self.base_url = 'https://maimai.cn'
        
    def crawl(self) -> List[JobData]:
        """çˆ¬å–è„‰è„‰å†…æ¨ä¿¡æ¯"""
        jobs = []
        
        # è„‰è„‰çš„æ±‚èŒæ¿å—
        sections = [
            'job_referral',  # å†…æ¨
            'campus_recruit', # æ ¡æ‹›
            'social_recruit', # ç¤¾æ‹›
            'internship'      # å®ä¹ 
        ]
        
        for section in sections:
            try:
                self.logger.info(f'çˆ¬å–æ¿å—: {section}')
                section_jobs = self.crawl_section(section)
                jobs.extend(section_jobs)
                self.random_delay(2, 4)
            except Exception as e:
                self.logger.error(f'çˆ¬å–æ¿å— {section} å¤±è´¥: {e}')
                continue
                
        return jobs
    
    def crawl_section(self, section: str) -> List[JobData]:
        """çˆ¬å–ç‰¹å®šæ¿å—çš„å†…æ¨ä¿¡æ¯"""
        jobs = []
        
        # æ¨¡æ‹Ÿè„‰è„‰çš„å†…æ¨å¸–å­
        sample_posts = [
            {
                'title': 'æ»´æ»´å‡ºè¡Œç®—æ³•å·¥ç¨‹å¸ˆå†…æ¨',
                'content': 'æ»´æ»´åœ°å›¾ç®—æ³•å›¢é˜Ÿæ‹›è˜ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£è·¯å¾„è§„åˆ’å’Œæ™ºèƒ½è°ƒåº¦ç®—æ³•å¼€å‘ã€‚è¦æ±‚ï¼šè®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼Œç†Ÿæ‚‰æœºå™¨å­¦ä¹ ï¼Œæœ‰åœ°å›¾æˆ–å‡ºè¡Œé¢†åŸŸç»éªŒä¼˜å…ˆã€‚',
                'author': 'æ»´æ»´HR-å¼ ç»ç†',
                'company': 'æ»´æ»´',
                'department': 'åœ°å›¾ç®—æ³•å›¢é˜Ÿ',
                'location': 'åŒ—äº¬',
                'job_level': 'ä¸­çº§'
            },
            {
                'title': 'å¿«æ‰‹çŸ­è§†é¢‘æ¨èç®—æ³•å®ä¹ ç”Ÿ',
                'content': 'å¿«æ‰‹æ¨èç®—æ³•å›¢é˜Ÿæ‹›å®ä¹ ç”Ÿï¼Œå‚ä¸çŸ­è§†é¢‘æ¨èç³»ç»Ÿä¼˜åŒ–ã€‚è¦æ±‚ï¼šåœ¨æ ¡å­¦ç”Ÿï¼Œè®¡ç®—æœº/æ•°å­¦ç›¸å…³ä¸“ä¸šï¼Œç†Ÿæ‚‰æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæœ‰æ¨èç³»ç»Ÿé¡¹ç›®ç»éªŒè€…ä¼˜å…ˆã€‚',
                'author': 'å¿«æ‰‹ç®—æ³•-æå·¥',
                'company': 'å¿«æ‰‹',
                'department': 'æ¨èç®—æ³•å›¢é˜Ÿ',
                'location': 'åŒ—äº¬',
                'job_level': 'å®ä¹ '
            },
            {
                'title': 'èš‚èšé›†å›¢åç«¯å¼€å‘å·¥ç¨‹å¸ˆç¤¾æ‹›',
                'content': 'èš‚èšé›†å›¢æ”¯ä»˜å®æ ¸å¿ƒç³»ç»Ÿå›¢é˜Ÿæ‹›åç«¯å¼€å‘ï¼Œè´Ÿè´£æ”¯ä»˜ç³»ç»Ÿæ¶æ„è®¾è®¡å’Œå¼€å‘ã€‚è¦æ±‚ï¼š5å¹´ä»¥ä¸ŠJavaå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæœ‰é‡‘èç³»ç»Ÿç»éªŒä¼˜å…ˆã€‚',
                'author': 'èš‚èšæŠ€æœ¯-ç‹æ€»ç›‘',
                'company': 'èš‚èšé›†å›¢',
                'department': 'æ”¯ä»˜æ ¸å¿ƒç³»ç»Ÿ',
                'location': 'æ­å·',
                'job_level': 'é«˜çº§'
            },
            {
                'title': 'ç½‘æ˜“äº‘éŸ³ä¹å‰ç«¯å¼€å‘æ ¡æ‹›',
                'content': 'ç½‘æ˜“äº‘éŸ³ä¹å‰ç«¯å›¢é˜Ÿ2025æ ¡æ‹›ï¼Œè´Ÿè´£éŸ³ä¹æ’­æ”¾å™¨å’Œç”¨æˆ·ç•Œé¢å¼€å‘ã€‚è¦æ±‚ï¼šæœ¬ç§‘ä»¥ä¸Šå­¦å†ï¼Œç†Ÿæ‚‰React/Vueï¼Œæœ‰éŸ³è§†é¢‘ç›¸å…³é¡¹ç›®ç»éªŒåŠ åˆ†ã€‚',
                'author': 'ç½‘æ˜“äº‘éŸ³ä¹-HR',
                'company': 'ç½‘æ˜“',
                'department': 'äº‘éŸ³ä¹å‰ç«¯å›¢é˜Ÿ',
                'location': 'æ­å·',
                'job_level': 'åˆçº§'
            },
            {
                'title': 'å°ç±³MIUIäº§å“ç»ç†å†…æ¨',
                'content': 'MIUIäº§å“å›¢é˜Ÿæ‹›äº§å“ç»ç†ï¼Œè´Ÿè´£ç³»ç»Ÿåº”ç”¨äº§å“è®¾è®¡å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ–ã€‚è¦æ±‚ï¼š3å¹´ä»¥ä¸Šäº§å“ç»éªŒï¼Œæœ‰ç§»åŠ¨ç«¯äº§å“è®¾è®¡ç»éªŒï¼Œç”¨æˆ·è°ƒç ”å’Œæ•°æ®åˆ†æèƒ½åŠ›å¼ºã€‚',
                'author': 'å°ç±³äº§å“-é™ˆç»ç†',
                'company': 'å°ç±³',
                'department': 'MIUIäº§å“å›¢é˜Ÿ',
                'location': 'åŒ—äº¬',
                'job_level': 'ä¸­çº§'
            }
        ]
        
        for post in sample_posts:
            try:
                job = self.parse_post(post)
                if job:
                    jobs.append(job)
            except Exception as e:
                self.logger.error(f'è§£æå¸–å­å¤±è´¥: {e}')
                continue
                
        return jobs
    
    def parse_post(self, post: dict) -> JobData:
        """è§£æè„‰è„‰å¸–å­æ•°æ®"""
        title = post.get('title', '')
        content = post.get('content', '')
        company = post.get('company', '')
        department = post.get('department', '')
        location = post.get('location', '')
        job_level = post.get('job_level', '')
        
        # ä»èŒçº§åˆ¤æ–­èŒä½ç±»å‹
        job_type = self.extract_job_type_from_level(job_level, title + content)
        
        # æå–æŠ€æœ¯æ–¹å‘
        direction = self.extract_direction(title, content)
        
        # ç”Ÿæˆå†…æ¨ç 
        referral_code = self.generate_referral_code(company, job_type)
        
        # æå–èŒä½æ ‡é¢˜
        position_title = self.extract_position_title(title, direction, job_level)
        
        # æå–è¦æ±‚
        requirements = self.extract_requirements(content, job_level)
        
        # å¢å¼ºæè¿°ä¿¡æ¯
        enhanced_description = self.enhance_description(content, department, location)
        
        return JobData(
            title=position_title,
            company=company,
            job_type=job_type,
            direction=direction,
            source='è„‰è„‰',
            code=referral_code,
            description=enhanced_description,
            requirements=requirements
        )
    
    def extract_job_type_from_level(self, job_level: str, content: str) -> str:
        """ä»èŒçº§å’Œå†…å®¹æå–èŒä½ç±»å‹"""
        if job_level == 'å®ä¹ ' or 'å®ä¹ ' in content:
            return 'å®ä¹ '
        elif any(word in content for word in ['æ ¡æ‹›', 'åº”å±Š', '2025æ ¡æ‹›', 'æ¯•ä¸šç”Ÿ']):
            return 'æ ¡æ‹›'
        else:
            return 'ç¤¾æ‹›'
    
    def extract_position_title(self, title: str, direction: str, job_level: str) -> str:
        """æå–èŒä½æ ‡é¢˜"""
        # ç§»é™¤å…¬å¸åç§°å’Œå†…æ¨ç­‰è¯æ±‡
        clean_title = re.sub(r'(å†…æ¨|æ‹›è˜|2025)', '', title)
        
        # æ ¹æ®æ–¹å‘å’Œçº§åˆ«ç¡®å®šèŒä½
        level_prefix = {
            'åˆçº§': 'åˆçº§',
            'ä¸­çº§': 'é«˜çº§', 
            'é«˜çº§': 'èµ„æ·±',
            'å®ä¹ ': 'å®ä¹ ç”Ÿ'
        }.get(job_level, '')
        
        if direction == 'ç®—æ³•':
            if 'æ¨è' in title:
                return f'{level_prefix}æ¨èç®—æ³•å·¥ç¨‹å¸ˆ'.strip()
            elif 'åœ°å›¾' in title or 'è·¯å¾„' in title:
                return f'{level_prefix}åœ°å›¾ç®—æ³•å·¥ç¨‹å¸ˆ'.strip()
            else:
                return f'{level_prefix}ç®—æ³•å·¥ç¨‹å¸ˆ'.strip()
        elif direction == 'åç«¯':
            if 'æ”¯ä»˜' in title:
                return f'{level_prefix}æ”¯ä»˜ç³»ç»Ÿåç«¯å·¥ç¨‹å¸ˆ'.strip()
            else:
                return f'{level_prefix}åç«¯å¼€å‘å·¥ç¨‹å¸ˆ'.strip()
        elif direction == 'å‰ç«¯':
            if 'éŸ³ä¹' in title:
                return f'{level_prefix}å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ(éŸ³ä¹)'
            else:
                return f'{level_prefix}å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ'.strip()
        elif direction == 'äº§å“':
            if 'MIUI' in title or 'ç³»ç»Ÿ' in title:
                return f'{level_prefix}ç³»ç»Ÿäº§å“ç»ç†'.strip()
            else:
                return f'{level_prefix}äº§å“ç»ç†'.strip()
        else:
            return f'{level_prefix}{direction}å·¥ç¨‹å¸ˆ'.strip()
    
    def extract_requirements(self, content: str, job_level: str) -> List[str]:
        """æå–èŒä½è¦æ±‚"""
        requirements = []
        
        # æ ¹æ®èŒçº§æ·»åŠ ç»éªŒè¦æ±‚
        if job_level == 'é«˜çº§':
            requirements.append('5å¹´ä»¥ä¸Šç›¸å…³å·¥ä½œç»éªŒ')
        elif job_level == 'ä¸­çº§':
            requirements.append('3å¹´ä»¥ä¸Šç›¸å…³å·¥ä½œç»éªŒ')
        elif job_level == 'åˆçº§':
            requirements.append('1-3å¹´ç›¸å…³å·¥ä½œç»éªŒ')
        elif job_level == 'å®ä¹ ':
            requirements.append('åœ¨æ ¡å­¦ç”Ÿï¼Œç›¸å…³ä¸“ä¸š')
        
        # å­¦å†è¦æ±‚
        if 'æœ¬ç§‘' in content:
            requirements.append('æœ¬ç§‘åŠä»¥ä¸Šå­¦å†')
        elif 'ç¡•å£«' in content or 'ç ”ç©¶ç”Ÿ' in content:
            requirements.append('ç¡•å£«åŠä»¥ä¸Šå­¦å†')
        
        # æŠ€æœ¯æŠ€èƒ½è¦æ±‚
        tech_skills = {
            'Java': 'Javaå¼€å‘ç»éªŒ',
            'æœºå™¨å­¦ä¹ ': 'æœºå™¨å­¦ä¹ åŸºç¡€',
            'æ·±åº¦å­¦ä¹ ': 'æ·±åº¦å­¦ä¹ æ¡†æ¶ç»éªŒ',
            'React': 'Reactå¼€å‘ç»éªŒ',
            'Vue': 'Vueå¼€å‘ç»éªŒ',
            'åˆ†å¸ƒå¼': 'åˆ†å¸ƒå¼ç³»ç»Ÿç»éªŒ',
            'Python': 'Pythonç¼–ç¨‹èƒ½åŠ›'
        }
        
        for skill, desc in tech_skills.items():
            if skill in content:
                requirements.append(desc)
        
        # é¢†åŸŸç»éªŒ
        domain_exp = {
            'é‡‘è': 'é‡‘èç³»ç»Ÿå¼€å‘ç»éªŒ',
            'æ”¯ä»˜': 'æ”¯ä»˜ç³»ç»Ÿç»éªŒ',
            'æ¨èç³»ç»Ÿ': 'æ¨èç³»ç»Ÿé¡¹ç›®ç»éªŒ',
            'åœ°å›¾': 'åœ°å›¾æˆ–GISç›¸å…³ç»éªŒ',
            'éŸ³è§†é¢‘': 'éŸ³è§†é¢‘å¤„ç†ç»éªŒ',
            'ç§»åŠ¨ç«¯': 'ç§»åŠ¨ç«¯äº§å“ç»éªŒ'
        }
        
        for domain, exp in domain_exp.items():
            if domain in content:
                requirements.append(exp)
        
        # è½¯æŠ€èƒ½
        if 'æ•°æ®åˆ†æ' in content:
            requirements.append('æ•°æ®åˆ†æèƒ½åŠ›')
        if 'ç”¨æˆ·è°ƒç ”' in content:
            requirements.append('ç”¨æˆ·ç ”ç©¶èƒ½åŠ›')
        if 'æ¶æ„è®¾è®¡' in content:
            requirements.append('ç³»ç»Ÿæ¶æ„è®¾è®¡èƒ½åŠ›')
        
        # é»˜è®¤è¦æ±‚
        if not requirements:
            requirements = ['ç›¸å…³ä¸“ä¸šèƒŒæ™¯', 'è‰¯å¥½çš„å­¦ä¹ èƒ½åŠ›', 'å›¢é˜Ÿåˆä½œç²¾ç¥']
        
        return requirements[:6]
    
    def enhance_description(self, content: str, department: str, location: str) -> str:
        """å¢å¼ºæè¿°ä¿¡æ¯"""
        enhanced = content
        
        if department:
            enhanced += f'\n\næ‰€å±éƒ¨é—¨ï¼š{department}'
        
        if location:
            enhanced += f'\nå·¥ä½œåœ°ç‚¹ï¼š{location}'
        
        # æ·»åŠ è„‰è„‰å¹³å°ç‰¹è‰²ä¿¡æ¯
        enhanced += '\n\nğŸ’¼ é€šè¿‡è„‰è„‰å¹³å°å†…æ¨ï¼Œå¯ç›´æ¥è”ç³»å†…æ¨äººäº†è§£æ›´å¤šèŒä½è¯¦æƒ…'
        
        return enhanced

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    crawler = MaimaiCrawler()
    jobs = crawler.run()
    print(f'çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(jobs)} ä¸ªèŒä½')

if __name__ == '__main__':
    main()
