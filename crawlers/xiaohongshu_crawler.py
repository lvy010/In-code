#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦çˆ¬è™«
çˆ¬å–å°çº¢ä¹¦çš„æ±‚èŒå†…æ¨ä¿¡æ¯
"""

import re
import json
from typing import List
from base_crawler import BaseCrawler, JobData

class XiaohongshuCrawler(BaseCrawler):
    """å°çº¢ä¹¦çˆ¬è™«"""
    
    def __init__(self):
        super().__init__('å°çº¢ä¹¦')
        self.base_url = 'https://www.xiaohongshu.com'
        
    def crawl(self) -> List[JobData]:
        """çˆ¬å–å°çº¢ä¹¦å†…æ¨ä¿¡æ¯"""
        jobs = []
        
        # å°çº¢ä¹¦æ±‚èŒç›¸å…³å…³é”®è¯
        keywords = [
            'å†…æ¨',
            'æ ¡æ‹›',
            'ç¤¾æ‹›', 
            'å®ä¹ ',
            'æ±‚èŒ',
            'æ‰¾å·¥ä½œ'
        ]
        
        for keyword in keywords:
            try:
                self.logger.info(f'æœç´¢å…³é”®è¯: {keyword}')
                keyword_jobs = self.search_keyword(keyword)
                jobs.extend(keyword_jobs)
                self.random_delay(2, 4)
            except Exception as e:
                self.logger.error(f'æœç´¢å…³é”®è¯ {keyword} å¤±è´¥: {e}')
                continue
                
        return jobs
    
    def search_keyword(self, keyword: str) -> List[JobData]:
        """æœç´¢ç‰¹å®šå…³é”®è¯çš„å†…æ¨ä¿¡æ¯"""
        jobs = []
        
        # æ¨¡æ‹Ÿå°çº¢ä¹¦çš„æ±‚èŒå†…æ¨ç¬”è®°
        sample_notes = [
            {
                'title': 'ğŸ”¥å­—èŠ‚è·³åŠ¨2025æ ¡æ‹›å†…æ¨ç æ¥å•¦ï¼',
                'content': 'å§å¦¹ä»¬ï¼å­—èŠ‚è·³åŠ¨ç®—æ³•å²—ä½å¼€æ”¾ç”³è¯·å•¦ğŸ‰ è´Ÿè´£æ¨èç®—æ³•ä¼˜åŒ–ï¼Œè¦æ±‚è®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼Œæœ‰æ·±åº¦å­¦ä¹ é¡¹ç›®ç»éªŒã€‚å·¥ä½œåœ°ç‚¹ï¼šåŒ—äº¬/ä¸Šæµ·ã€‚ç¦åˆ©è¶…å¥½çš„å“¦ï½',
                'author': 'tech_girl_123',
                'tags': ['æ±‚èŒ', 'å†…æ¨', 'ç®—æ³•', 'å­—èŠ‚è·³åŠ¨'],
                'company': 'å­—èŠ‚è·³åŠ¨'
            },
            {
                'title': 'è…¾è®¯äº§å“ç»ç†å†…æ¨ï½œåº”å±Šç”Ÿå‹å¥½ï¼',
                'content': 'è…¾è®¯å¾®ä¿¡å›¢é˜Ÿæ‹›äº§å“ç»ç†å•¦ï¼ä¸»è¦è´Ÿè´£ç”¨æˆ·ä½“éªŒä¼˜åŒ–å’Œäº§å“åŠŸèƒ½è®¾è®¡ã€‚è¦æ±‚ï¼šäº§å“æ€ç»´å¼ºï¼Œæœ‰æ•°æ®åˆ†æèƒ½åŠ›ï¼Œæ²Ÿé€šèƒ½åŠ›ä½³ã€‚baseæ·±åœ³ï¼Œæ°›å›´å¾ˆæ£’ï¼',
                'author': 'pm_xiaoli',
                'tags': ['äº§å“', 'è…¾è®¯', 'åº”å±Šç”Ÿ'],
                'company': 'è…¾è®¯'
            },
            {
                'title': 'é˜¿é‡Œå·´å·´å‰ç«¯å®ä¹ ç”Ÿå†…æ¨ï¼æŠ•é€’ä»é€Ÿ',
                'content': 'é˜¿é‡Œæ·˜å®å‰ç«¯å›¢é˜Ÿæ‹›å®ä¹ ç”ŸğŸ‘©â€ğŸ’» ä¸»è¦åšç§»åŠ¨ç«¯å¼€å‘ï¼ŒæŠ€æœ¯æ ˆReact+TypeScriptã€‚è¦æ±‚ç†Ÿæ‚‰å‰ç«¯åŸºç¡€ï¼Œæœ‰é¡¹ç›®ç»éªŒã€‚å®ä¹ æœŸ6ä¸ªæœˆï¼Œæœ‰è½¬æ­£æœºä¼šï¼',
                'author': 'frontend_dev',
                'tags': ['å‰ç«¯', 'å®ä¹ ', 'é˜¿é‡Œå·´å·´'],
                'company': 'é˜¿é‡Œå·´å·´'
            },
            {
                'title': 'ç¾å›¢åç«¯å¼€å‘ç¤¾æ‹›å†…æ¨ç ğŸ“®',
                'content': 'ç¾å›¢å¤–å–æŠ€æœ¯å›¢é˜Ÿæ‹›åç«¯å¼€å‘å·¥ç¨‹å¸ˆï¼Œä¸»è¦è´Ÿè´£è®¢å•ç³»ç»Ÿå¼€å‘ã€‚è¦æ±‚3å¹´ä»¥ä¸ŠJavaç»éªŒï¼Œç†Ÿæ‚‰Spring Bootã€MySQLã€Redisç­‰ã€‚è–ªèµ„å¯è°ˆï¼',
                'author': 'meituan_hr',
                'tags': ['åç«¯', 'ç¤¾æ‹›', 'ç¾å›¢'],
                'company': 'ç¾å›¢'
            },
            {
                'title': 'å°çº¢ä¹¦æ•°æ®åˆ†æå¸ˆå†…æ¨ï½œå¥³ç”Ÿå‹å¥½',
                'content': 'å°çº¢ä¹¦æ•°æ®å›¢é˜Ÿæ‹›æ•°æ®åˆ†æå¸ˆå•¦ğŸ’• è´Ÿè´£ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œå•†ä¸šæ•°æ®æŒ–æ˜ã€‚è¦æ±‚ç†Ÿæ‚‰SQLã€Pythonï¼Œæœ‰ç»Ÿè®¡å­¦èƒŒæ™¯ä¼˜å…ˆã€‚å·¥ä½œç¯å¢ƒå¾ˆæ£’ï¼Œå¥³ç”Ÿæ¯”ä¾‹é«˜ï¼',
                'author': 'xhs_data_team',
                'tags': ['æ•°æ®åˆ†æ', 'å°çº¢ä¹¦'],
                'company': 'å°çº¢ä¹¦'
            }
        ]
        
        for note in sample_notes:
            try:
                job = self.parse_note(note)
                if job:
                    jobs.append(job)
            except Exception as e:
                self.logger.error(f'è§£æç¬”è®°å¤±è´¥: {e}')
                continue
                
        return jobs
    
    def parse_note(self, note: dict) -> JobData:
        """è§£æå°çº¢ä¹¦ç¬”è®°æ•°æ®"""
        title = note.get('title', '')
        content = note.get('content', '')
        company = note.get('company', '')
        tags = note.get('tags', [])
        
        # æ¸…ç†æ ‡é¢˜ä¸­çš„emojiå’Œç‰¹æ®Šå­—ç¬¦
        clean_title = self.clean_title(title)
        
        # ä»æ ‡ç­¾å’Œå†…å®¹ä¸­æå–èŒä½ç±»å‹
        job_type = self.extract_job_type_from_tags(tags, title + content)
        
        # æå–æŠ€æœ¯æ–¹å‘
        direction = self.extract_direction(title, content)
        
        # ç”Ÿæˆå†…æ¨ç 
        referral_code = self.generate_referral_code(company, job_type)
        
        # æå–è¦æ±‚
        requirements = self.extract_requirements(content)
        
        # æå–èŒä½æ ‡é¢˜
        position_title = self.extract_position_title(title, content, direction)
        
        return JobData(
            title=position_title,
            company=company,
            job_type=job_type,
            direction=direction,
            source='å°çº¢ä¹¦',
            code=referral_code,
            description=content,
            requirements=requirements
        )
    
    def clean_title(self, title: str) -> str:
        """æ¸…ç†æ ‡é¢˜ä¸­çš„emojiå’Œç‰¹æ®Šç¬¦å·"""
        # ç§»é™¤emoji
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map
            "\U0001F1E0-\U0001F1FF"  # flags
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE
        )
        title = emoji_pattern.sub('', title)
        
        # ç§»é™¤ç‰¹æ®Šç¬¦å·
        title = re.sub(r'[ğŸ”¥ğŸ’•ğŸ‘©â€ğŸ’»ğŸ“®ï½œï¼]', '', title)
        
        return title.strip()
    
    def extract_job_type_from_tags(self, tags: List[str], content: str) -> str:
        """ä»æ ‡ç­¾å’Œå†…å®¹ä¸­æå–èŒä½ç±»å‹"""
        tags_str = ' '.join(tags).lower()
        content_lower = content.lower()
        
        if 'å®ä¹ ' in tags_str or 'å®ä¹ ' in content_lower:
            return 'å®ä¹ '
        elif any(word in tags_str or word in content_lower for word in ['æ ¡æ‹›', 'åº”å±Š', 'æ¯•ä¸šç”Ÿ']):
            return 'æ ¡æ‹›'
        else:
            return 'ç¤¾æ‹›'
    
    def extract_position_title(self, title: str, content: str, direction: str) -> str:
        """æå–èŒä½æ ‡é¢˜"""
        content_lower = content.lower()
        
        # æ ¹æ®æŠ€æœ¯æ–¹å‘å’Œå†…å®¹ç¡®å®šå…·ä½“èŒä½
        if direction == 'ç®—æ³•':
            if 'æ¨è' in content:
                return 'æ¨èç®—æ³•å·¥ç¨‹å¸ˆ'
            elif 'nlp' in content_lower or 'è‡ªç„¶è¯­è¨€' in content:
                return 'NLPç®—æ³•å·¥ç¨‹å¸ˆ'
            else:
                return 'ç®—æ³•å·¥ç¨‹å¸ˆ'
        elif direction == 'å‰ç«¯':
            if 'ç§»åŠ¨ç«¯' in content or 'mobile' in content_lower:
                return 'ç§»åŠ¨ç«¯å‰ç«¯å·¥ç¨‹å¸ˆ'
            else:
                return 'å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ'
        elif direction == 'åç«¯':
            if 'è®¢å•' in content:
                return 'åç«¯å¼€å‘å·¥ç¨‹å¸ˆ(è®¢å•ç³»ç»Ÿ)'
            else:
                return 'åç«¯å¼€å‘å·¥ç¨‹å¸ˆ'
        elif direction == 'äº§å“':
            if 'ç”¨æˆ·ä½“éªŒ' in content:
                return 'ç”¨æˆ·ä½“éªŒäº§å“ç»ç†'
            else:
                return 'äº§å“ç»ç†'
        elif direction == 'æ•°æ®':
            if 'å•†ä¸š' in content:
                return 'å•†ä¸šæ•°æ®åˆ†æå¸ˆ'
            else:
                return 'æ•°æ®åˆ†æå¸ˆ'
        else:
            return f'{direction}å·¥ç¨‹å¸ˆ'
    
    def extract_requirements(self, content: str) -> List[str]:
        """æå–èŒä½è¦æ±‚"""
        requirements = []
        
        # å­¦å†è¦æ±‚
        if 'ç¡•å£«' in content or 'ç ”ç©¶ç”Ÿ' in content:
            requirements.append('ç¡•å£«åŠä»¥ä¸Šå­¦å†')
        elif 'æœ¬ç§‘' in content or 'å­¦å£«' in content:
            requirements.append('æœ¬ç§‘åŠä»¥ä¸Šå­¦å†')
        
        # ç»éªŒè¦æ±‚
        exp_patterns = [
            r'(\d+å¹´ä»¥ä¸Š.+?ç»éªŒ)',
            r'(æœ‰.+?é¡¹ç›®ç»éªŒ)',
            r'(ç†Ÿæ‚‰.+?)(?:[ï¼Œã€‚]|$)'
        ]
        
        for pattern in exp_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                req = match.strip().rstrip('ï¼Œã€‚')
                if req and len(req) > 2:
                    requirements.append(req)
        
        # æŠ€èƒ½è¦æ±‚
        skills = {
            'React': 'Reactå¼€å‘ç»éªŒ',
            'TypeScript': 'TypeScriptå¼€å‘ç»éªŒ', 
            'Java': 'Javaå¼€å‘ç»éªŒ',
            'Spring Boot': 'Spring Bootæ¡†æ¶ç»éªŒ',
            'MySQL': 'æ•°æ®åº“æ“ä½œç»éªŒ',
            'Redis': 'ç¼“å­˜æŠ€æœ¯ç»éªŒ',
            'SQL': 'SQLæŸ¥è¯¢èƒ½åŠ›',
            'Python': 'Pythonç¼–ç¨‹èƒ½åŠ›'
        }
        
        content_lower = content.lower()
        for skill, desc in skills.items():
            if skill.lower() in content_lower:
                requirements.append(desc)
        
        # è½¯æŠ€èƒ½è¦æ±‚
        if 'æ²Ÿé€š' in content:
            requirements.append('è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›')
        if 'æ•°æ®åˆ†æ' in content:
            requirements.append('æ•°æ®åˆ†æèƒ½åŠ›')
        if 'äº§å“æ€ç»´' in content:
            requirements.append('äº§å“æ€ç»´èƒ½åŠ›')
        
        # é»˜è®¤è¦æ±‚
        if not requirements:
            requirements = ['ç›¸å…³ä¸“ä¸šèƒŒæ™¯', 'å­¦ä¹ èƒ½åŠ›å¼º', 'å·¥ä½œè´£ä»»å¿ƒå¼º']
        
        return requirements[:5]

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    crawler = XiaohongshuCrawler()
    jobs = crawler.run()
    print(f'çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(jobs)} ä¸ªèŒä½')

if __name__ == '__main__':
    main()
