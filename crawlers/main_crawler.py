#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»çˆ¬è™«è„šæœ¬
åè°ƒè¿è¡Œæ‰€æœ‰å¹³å°çš„çˆ¬è™«ï¼Œå¹¶æ•´åˆæ•°æ®
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from base_crawler import JobData
from nowcoder_crawler import NowcoderCrawler
from leetcode_crawler import LeetcodeCrawler
from xiaohongshu_crawler import XiaohongshuCrawler
from maimai_crawler import MaimaiCrawler
from real_data_crawler import RealDataCrawler

class MainCrawler:
    """ä¸»çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger('main_crawler')
        self.crawlers = {
            'ç‰›å®¢': NowcoderCrawler(),
            'åŠ›æ‰£': LeetcodeCrawler(),
            'å°çº¢ä¹¦': XiaohongshuCrawler(),
            'è„‰è„‰': MaimaiCrawler(),
            'çœŸå®æ•°æ®': RealDataCrawler()  # æ–°å¢çœŸå®æ•°æ®çˆ¬è™«
        }
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        
        # ç¡®ä¿å‰ç«¯æ•°æ®ç›®å½•å­˜åœ¨
        self.frontend_data_dir = Path('../data')
        self.frontend_data_dir.mkdir(exist_ok=True)
    
    def run_all_crawlers(self) -> Dict[str, List[JobData]]:
        """è¿è¡Œæ‰€æœ‰çˆ¬è™«"""
        self.logger.info('å¼€å§‹è¿è¡Œæ‰€æœ‰çˆ¬è™«...')
        start_time = time.time()
        
        all_jobs = {}
        total_jobs = 0
        
        for platform, crawler in self.crawlers.items():
            try:
                self.logger.info(f'è¿è¡Œ {platform} çˆ¬è™«...')
                jobs = crawler.run()
                all_jobs[platform] = jobs
                total_jobs += len(jobs)
                
                self.logger.info(f'{platform} çˆ¬è™«å®Œæˆï¼Œè·å– {len(jobs)} ä¸ªèŒä½')
                
                # ä¼‘æ¯ä¸€ä¸‹å†è¿è¡Œä¸‹ä¸€ä¸ªçˆ¬è™«
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f'{platform} çˆ¬è™«è¿è¡Œå¤±è´¥: {e}')
                all_jobs[platform] = []
        
        end_time = time.time()
        duration = end_time - start_time
        
        self.logger.info(f'æ‰€æœ‰çˆ¬è™«è¿è¡Œå®Œæˆï¼Œæ€»å…±è·å– {total_jobs} ä¸ªèŒä½ï¼Œè€—æ—¶ {duration:.2f} ç§’')
        
        return all_jobs
    
    def merge_and_save_data(self, all_jobs: Dict[str, List[JobData]]):
        """åˆå¹¶å¹¶ä¿å­˜æ‰€æœ‰æ•°æ®"""
        self.logger.info('å¼€å§‹åˆå¹¶å’Œä¿å­˜æ•°æ®...')
        
        # åˆå¹¶æ‰€æœ‰èŒä½æ•°æ®
        merged_jobs = []
        for platform, jobs in all_jobs.items():
            for job in jobs:
                merged_jobs.append(job.to_dict())
        
        # æŒ‰æ—¥æœŸæ’åº
        merged_jobs.sort(key=lambda x: x['date'], reverse=True)
        
        # ä¿å­˜åˆ°æ•°æ®æ–‡ä»¶
        today = datetime.now().strftime('%Y%m%d')
        
        # ä¿å­˜åˆ°çˆ¬è™«æ•°æ®ç›®å½•
        crawler_data_file = self.data_dir / f'all_jobs_{today}.json'
        with open(crawler_data_file, 'w', encoding='utf-8') as f:
            json.dump(merged_jobs, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆ°å‰ç«¯æ•°æ®ç›®å½•ï¼ˆç”¨äºç½‘ç«™æ˜¾ç¤ºï¼‰
        frontend_data_file = self.frontend_data_dir / 'jobs.json'
        with open(frontend_data_file, 'w', encoding='utf-8') as f:
            json.dump(merged_jobs, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f'æ•°æ®ä¿å­˜å®Œæˆ: {len(merged_jobs)} ä¸ªèŒä½')
        self.logger.info(f'çˆ¬è™«æ•°æ®æ–‡ä»¶: {crawler_data_file}')
        self.logger.info(f'å‰ç«¯æ•°æ®æ–‡ä»¶: {frontend_data_file}')
        
        return merged_jobs
    
    def generate_statistics(self, merged_jobs: List[Dict]):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info('ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...')
        
        stats = {
            'total_jobs': len(merged_jobs),
            'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'by_source': {},
            'by_type': {},
            'by_direction': {},
            'by_company': {},
            'today_jobs': 0
        }
        
        today = datetime.now().strftime('%Y-%m-%d')
        
        for job in merged_jobs:
            # æŒ‰æ¥æºç»Ÿè®¡
            source = job.get('source', 'æœªçŸ¥')
            stats['by_source'][source] = stats['by_source'].get(source, 0) + 1
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            job_type = job.get('type', 'æœªçŸ¥')
            stats['by_type'][job_type] = stats['by_type'].get(job_type, 0) + 1
            
            # æŒ‰æ–¹å‘ç»Ÿè®¡
            direction = job.get('direction', 'æœªçŸ¥')
            stats['by_direction'][direction] = stats['by_direction'].get(direction, 0) + 1
            
            # æŒ‰å…¬å¸ç»Ÿè®¡
            company = job.get('company', 'æœªçŸ¥')
            stats['by_company'][company] = stats['by_company'].get(company, 0) + 1
            
            # ä»Šæ—¥æ–°å¢ç»Ÿè®¡
            if job.get('date') == today:
                stats['today_jobs'] += 1
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.frontend_data_dir / 'statistics.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f'ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}')
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        self.print_statistics_summary(stats)
        
        return stats
    
    def print_statistics_summary(self, stats: Dict):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦")
        print("="*50)
        print(f"ğŸ“… æ›´æ–°æ—¶é—´: {stats['update_time']}")
        print(f"ğŸ“¦ èŒä½æ€»æ•°: {stats['total_jobs']}")
        print(f"ğŸ†• ä»Šæ—¥æ–°å¢: {stats['today_jobs']}")
        
        print("\nğŸ“ˆ æŒ‰æ¥æºåˆ†å¸ƒ:")
        for source, count in stats['by_source'].items():
            print(f"  {source}: {count} ä¸ªèŒä½")
        
        print("\nğŸ·ï¸ æŒ‰ç±»å‹åˆ†å¸ƒ:")
        for job_type, count in stats['by_type'].items():
            print(f"  {job_type}: {count} ä¸ªèŒä½")
        
        print("\nğŸ’» æŒ‰æŠ€æœ¯æ–¹å‘åˆ†å¸ƒ:")
        for direction, count in stats['by_direction'].items():
            print(f"  {direction}: {count} ä¸ªèŒä½")
        
        print("\nğŸ¢ çƒ­é—¨å…¬å¸:")
        sorted_companies = sorted(stats['by_company'].items(), key=lambda x: x[1], reverse=True)
        for company, count in sorted_companies[:10]:  # æ˜¾ç¤ºå‰10å
            print(f"  {company}: {count} ä¸ªèŒä½")
        
        print("="*50)
    
    def cleanup_old_data(self, keep_days: int = 7):
        """æ¸…ç†æ—§æ•°æ®æ–‡ä»¶"""
        self.logger.info(f'æ¸…ç† {keep_days} å¤©å‰çš„æ•°æ®æ–‡ä»¶...')
        
        current_time = time.time()
        cutoff_time = current_time - (keep_days * 24 * 60 * 60)
        
        cleaned_count = 0
        for file_path in self.data_dir.glob('*.json'):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                cleaned_count += 1
                self.logger.info(f'åˆ é™¤æ—§æ–‡ä»¶: {file_path.name}')
        
        if cleaned_count > 0:
            self.logger.info(f'å·²æ¸…ç† {cleaned_count} ä¸ªæ—§æ•°æ®æ–‡ä»¶')
        else:
            self.logger.info('æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ–‡ä»¶')
    
    def run(self, cleanup_old: bool = True):
        """è¿è¡Œä¸»çˆ¬è™«æµç¨‹"""
        self.logger.info('ğŸš€ å¯åŠ¨å†…æ¨ç çˆ¬è™«ç³»ç»Ÿ')
        
        try:
            # 1. è¿è¡Œæ‰€æœ‰çˆ¬è™«
            all_jobs = self.run_all_crawlers()
            
            # 2. åˆå¹¶å’Œä¿å­˜æ•°æ®
            merged_jobs = self.merge_and_save_data(all_jobs)
            
            # 3. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self.generate_statistics(merged_jobs)
            
            # 4. æ¸…ç†æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
            if cleanup_old:
                self.cleanup_old_data()
            
            self.logger.info('âœ… çˆ¬è™«ç³»ç»Ÿè¿è¡Œå®Œæˆ')
            return merged_jobs, stats
            
        except Exception as e:
            self.logger.error(f'âŒ çˆ¬è™«ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}')
            raise

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('crawler_main.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # è¿è¡Œä¸»çˆ¬è™«
    crawler = MainCrawler()
    jobs, stats = crawler.run()
    
    print(f"\nğŸ‰ çˆ¬è™«è¿è¡Œå®Œæˆï¼å…±è·å– {len(jobs)} ä¸ªå†…æ¨èŒä½")
    print("ğŸ’¡ æç¤º: æ•°æ®å·²ä¿å­˜åˆ° ../data/jobs.jsonï¼Œå‰ç«¯ç½‘ç«™å°†è‡ªåŠ¨æ˜¾ç¤ºæœ€æ–°æ•°æ®")

if __name__ == '__main__':
    main()
